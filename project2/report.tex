\documentclass[12pt]{article}
\usepackage[swedish]{babel}
\usepackage{booktabs, float, listings, mathtools, tabu}
\usepackage[margin=2.5cm]{geometry}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\title{TANA21: Implementation av sekantmetoden i Matlab}
\author{Emil Segerbäck \and Olav Övrebö}

\begin{document}
\clearpage
\maketitle
\thispagestyle{empty}

\newpage
\setcounter{page}{1}

\section{Inledning}
För att finna rötter $x$ till en funktion $f$ finns det flera metoder man kan använda sig av. Sekantmetoden är en iterativ numerisk metod som tar fram godtyckligt bra approximationer av rötter.

\section{Uppgift}
Sekantmetoden ska implementeras som en Matlabfunktion och analyseras för att avgöra dess konvergensordning. Metoden skall utvärderas med avseende på diverse skilda funktioner.

\section{Teori}
Sekantmetoden används för att ta fram approximationer av rötter till en given funktion $f(x)$ med godtyckligt litet fel. Metoden uppnår detta genom att att approximera funktioner med en sekantlinje mellan två punkter $x_0$ och $x_1$ som antas utgöra grova uppskattningar av en faktisk rot, för att ta fram allt bättre approximationer. Specifikt gäller att sekantmetoden utgör en talserie för $x_n$ enligt

\[
x_{n} = x_{n-1} - \frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})} \cdot f(x_{n-1})
\]

där $x_{n}$ är en allt bättre approximation för högre värden på heltalet $n$. För implementation av metoden införs sedan ett slutvilkor att $|x_{n} - x_{n-1}| < E$, för någon övre feltolerans $E$.

Den skiljer sig således från Newton Raphsons metod, en annan vanlig iterativ approximeringsmetod, som istället använder tangentlinjer och således kräver att derivatan $f'$ är känd. Sekantmetoden undgår alltså detta krav genom att använda en sämre korrigering i varje iterationssteg. Då bägge metoder är iterativa och kan nå godtyckligt bra approximationer medför detta dock endast att sekantmetoden riskerar kräva fler iterationer för att nå ett resultat av jämförbar noggrannhet. Med andra ord väntas konvergensordningen vara något lägre än för Newton Raphsons metod (som vi från kurslitteraturen vet har konvergensordning 2).

\subsection{Konvergensordning}
Om $\overset{*}{x}$ är det exakta värdet av $x$ antas metoden konvergera enligt 
\[
  \abs*{x_{n-1}-\overset{*}{x}}\leq C\abs*{x_n-\overset{*}{x}}^p
\]
för någon konstant $C$. Alltså att den har konvergensordning $p$. För att uppskatta $p$ kan Ekvation~\ref{ekv:konvergens} användas som bör konvergera till värdet av $p$.

\begin{equation}
  p\approx\dfrac{\log\abs*{\dfrac{x_{n+1}-x_n}{x_n-x_{n-1}}}}{\log\abs*{\dfrac{x_n-x_{n-1}}{x_{n-1}-x_{n-2}}}}
\label{ekv:konvergens}
\end{equation}

\section{Lösning}
För att analysera sekantmetoden implementerades den rekursivt i Matlab; koden för detta redovisas under Avsnitt \ref{resultat}. 

För att testa den skrivna funktionen användes den för att uppskatta ett antal rötter till funktionerna $\sin(x)$, $ (x+2) x (x-3)$, $x^2$ och $\frac{1}{x} - 1$ för olika startpunkter. Resultaten redovisas i Tabell~\ref{noggrannhet} i Avsnitt \ref{resultat}. De faktiska rötterna är kända, och används i redovisningen för att ta fram felet. 

\section{Resultat och svar} \label{resultat}
Den färdiga implementationen utgörs av följande Matlab-funktion.
\begin{lstlisting}
function [root] = mysol(func, x0, x1, accuracy)
    korr = (x1 - x0) * func(x1)/(func(x1) - func(x0));
    if abs(korr) > accuracy
        root = mysol(func, x1, x1-korr, accuracy);
    else
        root = x1 - korr;
    end
end
\end{lstlisting}

Som Tabell~\ref{noggrannhet} visar gav de två första funktionerna ett resultat inom de efterfrågade feltoleranserna för samtliga startvärden. Dock gäller att metoden falerade för $f(x) = x^2$ och $f(x)=\frac{1}{x} - 1$.
\begin{table}[H]
  \centering
  \begin{tabu}{ l l l l l l l l }
    Funktion & $x_0$ & $x_1$ & Tolerans & Förväntat resultat & Resultat $\approx$ & Erhållet fel $\leq$ \\
    \toprule
    $\sin(x)$ & 0.8 & 1 & 0.01 & 0 & $2.15 \cdot 10^{-7}$ & $10^{-6}$ \\
    $\sin(x)$ & 4 & 3.8 & 0.01 & $\pi$ & 3.14 & $10^{-9} $\\
    $ (x+2) x (x-3)$ & -1 & 1 & 0.1 & 0 & $2.81 \cdot 10^{-4}$ & $10^{-3}$ \\
    $ (x+2) x (x-3)$ & 4 & 2 & 0.1 & 3 & 3 & $10^{-2}$ \\
    $ (x+2) x (x-3)$ & -10 & -5 & 0.1 & -2 & -2.02 & $10^{-1}$ \\
    $ x^{2}$ & -0.2 & 0.5 & 0.1 & 0 & -0.2  & $10^{0} $ \\
    $ \frac{1}{x} - 1 $ & 0.1 & 1.5 & 0.1 & 1 & 1.55 & $10^{0}$ \\
  \end{tabu}
  \caption{Testning av noggrannhet}\label{noggrannhet}
\end{table}

För att testa hur metoden konvergerar antecknades värdena för varje iteration ($1 < n \leq 8$) med funktionen $f(x)=(x+2) x (x-3)$ och startvärdena $x_0=-2.5$ och $x_1=-1.5$ i Tabell~\ref{konvergens}. Siffrorna tyder till viss del på en konvergensordning något under $p\approx 2$. Vid den nionde iterationen uppnåddes ett exakt värde för roten vilket innebar att fler iterationer inte kunde göras.

\begin{table}[H]
  \centering
  \begin{tabu}{l l l l l}
    n & $x_n$ & $\abs{x_{n+1}-x_n}$ & $\log\abs*{\dfrac{x_{n+1}-x_n}{x_n-x_{n-1}}}$ & uppskattning av $p$\\
    \toprule
    1 & -1.5  & 0.33 \\
    2 & -1.89 & 0.27     & -0.21  & \\
    3 & -2.10 & 0.11     & -0.91  & -0.55 \\
    4 & -1.99 & 0.01     & -2.27  & 4.43 \\
    5 & -2.00 & 7.63e-03 & -2.68  & 1.17 \\
    6 & -2.00 & 6.33e-06 & -4.79  & 1.83 \\
    7 & -2.00 & 3.35e-09 & -7.54  & 1.58 \\
    8 & -2.00 & 0        & -12.33 & 1.63 \\
    9 & 2
  \end{tabu}
  \caption{Testning av konvergens}\label{konvergens}
\end{table}

\section{Diskussion}
Som väntat konvergerar sekantmetoden långsamare än Newton Raphsons metod. Dock erhölls inget entydigt resultat. De otydliga resultaten förklaras med att förhållandevis få iterationer kan köras innan det att felet blir för små för att hantera i Matlab. Om fler iterationer kunde göras kunde konvergensordningen bestämmas tydligare.

% ; vidare läsning har dock visat att konvergensordningen bör fås som det gyllene snittet ($\approx 1.6803$). Möjligen anknyter detta till metodens likhet med gyllene snitt-sökning, en iterativ metod för approximativ bestämning av lokala extremvärden.

Tydligt var också att metoden var ytterst opålitlig; sekantmetoden falerar för dåligt valda startpunkter, såväl som dubbelrötter. Detta är föga förvånande då det i dessa fall gäller att sekanten blir en dålig approximation av funktionskurvan på det relevanta intervallet, vilket driver rotapproximationerna mot ett felaktigt sökområde där ingen rot sökes (eller nödvändigtvis finns). Att begränsa startvärderna så att de nödvändigtvis måste befinna sig på skilda sidor om den sökta roten skulle kunna motverka denna effekt, men riskerar att göra att metoden konvergerar långsammare då suboptimala approximationer görs. Detta är grundtanken bakom vissa andra iterativa metoder, såsom regula falsi.

\end{document}
