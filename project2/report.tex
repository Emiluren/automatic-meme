\documentclass{article}
\usepackage[swedish]{babel}
\usepackage{booktabs, float, listings, mathtools, tabu}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\title{TANA21: Implementation sekantmetoden i Matlab}
\author{Emil Segerbäck \and Olav Övrebö}

\begin{document}
\maketitle
\newpage

\section{Inledning}
För att finna rötter $x$ till en funktion $f$ finns det flera metoder man kan använda sig av. Sekantmetoden är en iterativ numerisk metod som tar fram godtyckligt bra apporximationer av rötter.

\section{Uppgift}
Sekantmetoden ska implementeras som en Matlab-funktion och analyseras för att avgöra dess konvergensordning.

\section{Teori}
Sekantmetoden används för att ta fram approximationer av rötter till en given funktion $f$ med godtyckligt litet fel. Metoden uppnår detta genom att att approximera funktioner med en sekantlinje mellan två punkter antaget liggandes nära en faktisk rot, för att ta fram allt bättre approximationer, och sedan upprepas med denna bättre approximation som en av de två punkterna, till det att felet är tillräckligt litet.

Den skiljer sig från Newton Raphsons metod, en annan vanlig iterativ approximeringsmetod, som istället använder tangentlinjer och således kräver att derivatan $f'$ är känd. Sekantmetoden undgår alltså detta krav genom att använda en sämre korrigering i varje iterationssteg. Då bägge metoder är iterativa och kan nå godtyckligt bra approximationer medför detta dock endast att sekantmetoden riskerar kräva fler iterationer för att nå ett resultat av jämförbar noggrannhet.

Specifikt gäller att sekantmetoden utgör en talserie givet en funktion $f$ och två startvärden $n_{0}, n_{1}$ (lämpligen placerade nära den önskade roten): 

$ n_{i} = n_{i-1} - \frac{n_{i-1} - n_{i-2}}{f(n_{i-1}) - f(n_{i-2})} \cdot f(n_{i-1}), i = 2, 3, ...$

Där $n_{i}$ är en allt bättre approximation för högre värden på $i$. För implementation av metoden införs sedan ett slutvilkor att $|n_{i} - n_{i-1}| < E$, för någon övre feltolerans $E$.

\section{Lösning}
För att analysera sekantmetoden implementerades den rekursivt i Matlab; koden för detta redovisas under avsnitt \ref{resultat}. 

För att testa den skrivna funktionen användes den för att bestämma ett antal rötter till funktionerna $\sin(x)$, $ (x+2) x (x-3)$, $x^2$ och $\frac{1}{x} - 1$ för olika startpunkter. Resultaten redovisas i Tabell~\ref{noggrannhet} i avsnitt \ref{resultat}. 

\section{Resultat och svar} \label{resultat}
Den färdiga implementationen utgörs av följande Matlab-funktion.
\begin{lstlisting}
function [root] = mysol(func, x0, x1, accuracy)
    korr = (x1 - x0) * func(x1)/(func(x1) - func(x0));
    if abs(korr) > accuracy
        root = mysol(func, x1, x1-korr, accuracy);
    else
        root = x1 - korr;
    end
end
\end{lstlisting}

\subsection{Noggrannhet}
Som tabellen visar gav alla de två första funktionerna ett resultat inom de efterfrågade feltoleranserna för samtliga startvärden. Dock gäller att metoden falerade för $f(x) = x^2$ och $\frac{1}{x} - 1$.
\begin{table}[H]
  \begin{tabu}{ l l l l X[0.5cm] X l X }
    Funktion & $x_0$ & $x_1$ & Tolerans & Förväntat resultat & Resultat $\approx$ & Erhållet fel $\leq$ \\
    \toprule
    $\sin(x)$ & 0.8 & 1 & 0.01 & 0 & $2.1544 \cdot 10^{-7}$ & $10^{-6}$ \\
    $\sin(x)$ & 4 & 3.8 & 0.01 & $\pi$ & 3.1416 & $10^{-9} $\\
    $ (x+2) x (x-3)$ & -1 & 1 & 0.1 & 0 & $2.8160 \cdot 10^{-4}$ & $10^{-3}$ \\
    $ (x+2) x (x-3)$ & 4 & 2 & 0.1 & 3 & 3 & $10^{-2}$ \\
    $ (x+2) x (x-3)$ & -10 & -5 & 0.1 & -2 & -2.0156 & $10^{-1}$ \\
    $ x^{2}$ & -0.2 & 0.5 & 0.1 & 0 & -0.2  & $10^{0} $ \\
    $ \frac{1}{x} - 1 $ & 0.1 & 1.5 & 0.1 & 1 & 1.55 & $10^{0}$ \\
  \end{tabu}
  \caption{Testning av noggrannhet}\label{noggrannhet}
\end{table}

\subsection{Konvergensordning}
Metoden antas konvergera enligt $\abs*{x_{n-1}-\overset{*}{x}}\leq C\abs*{x_n-\overset{*}{x}}^p$ (för någon konstant $C$). Alltså att den har konvergensordning $p$. För att uppskatta p kan Ekvation~\ref{ekv:konvergens} användas som bör konvergera till värdet av q.

\begin{equation}
  p\approx\dfrac{\log\abs*{\dfrac{x_{n+1}-x_n}{x_n-x_{n-1}}}}{\log\abs*{\dfrac{x_n-x_{n-1}}{x_{n-1}-x_{n-2}}}}
\label{ekv:konvergens}
\end{equation}

För att testa hur metoden konvergerar antecknades värdena för varje iteration $1 < n \leq 8$ för värdena $f(x)=(x+2) x (x-3)$, $x_0=-2.5$, $x_1=-1.5$ i Tabell~\ref{konvergens}. Siffrorna tyder till viss del på en konvergensordning något under $p\approx 2$.

\begin{table}[H]
  \begin{tabu}{l l l l l}
    n & $x_n$ & $\abs{x_{n+1}-x_n}$ & $\log\abs*{\dfrac{x_{n+1}-x_n}{x_n-x_{n-1}}}$ & uppskattning av $p$\\
    \toprule
    1 & -1.5    & 0.32927 \\
    2 & -1.8293 & 0.26603     & -0.21325 & \\
    3 & -2.0953 & 0.10717     & -0.90921 & -0.54613 \\
    4 & -1.9881 & 0.011111    & -2.2665  & 4.4288 \\
    5 & -1.9992 & 0.00076268  & -2.6789  & 1.1741 \\
    6 & -2.0000 & 6.3311e-06  & -4.7914  & 1.8285 \\
    7 & -1.9999 & 3.3516e-09  & -7.5438  & 1.5769 \\
    8 & -1.9999 & 0           & -12.3251 & 1.6339 \\
    9 & 2
  \end{tabu}
  \caption{Testning av konvergens}\label{konvergens}
\end{table}

\section{Diskussion}
Som väntat konvergerar sekantmetoden långsamare än Newton-Raphsons metod. Mindre väntat var att vi erhöll konvergensordningen som varandes det gyllene snittet. Möjligen anknyter detta till metodens likhet med så kallad gyllene snitt-sökning, en itrerativ metod för approximation av lokala extremvärden. 

Tydligt var också att metoden var ytterst opålitlig; sekantmetoden falerar för dåligt valda startpunkter, och dubbelrötter. Detta är föga förvånande då det i dessa fall gäller att sekanten blir en dålig approximationa av funktionskurvan på det relevanta intervallet, vilket driver rotapproximationerna mot ett felaktigt värde, där ingen rot sökes (eller nödvändigtvis finns). Att begränsa startvärderna så att de nödvändigtvis måste befinna sig på skilda sidor om den sökta roten skulle kunna motverka denna effekt, men riskerar att göra att metoden konvergerar långsammare då suboptimala approximationer görs. Detta är grundtanken bakom vissa andra iterativa metoder, såsom regula falsi.

\end{document}
